{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7983b807",
   "metadata": {},
   "source": [
    "### Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9340c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e501966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-17 08:26:13.400441: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(3,), name='my_input')\n",
    "features = layers.Dense(64, activation='relu')(inputs)\n",
    "outputs = layers.Dense(10, activation='softmax')(features)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff4672bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b92ffc4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float32"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aaacddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 64])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01866326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " my_input (InputLayer)       [(None, 3)]               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                256       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 906\n",
      "Trainable params: 906\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd9442f",
   "metadata": {},
   "source": [
    "Multi-input model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcd3312b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 10000\n",
    "num_tags = 100\n",
    "num_departments = 4\n",
    "\n",
    "title = keras.Input(shape=(vocabulary_size,), name='title')\n",
    "text_body = keras.Input(shape=(vocabulary_size,), name='text_body')\n",
    "tags = keras.Input(shape=(num_tags,), name='tags')\n",
    "\n",
    "features = layers.Concatenate()([title, text_body, tags])\n",
    "features = layers.Dense(64, activation='relu')(features)\n",
    "\n",
    "priority = layers.Dense(1, activation='sigmoid', name='priority')(features)\n",
    "department = layers.Dense(num_departments, activation='softmax', name='department')(features)\n",
    "\n",
    "model = keras.Model(inputs=[title, text_body, tags], outputs=[priority, department])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "437e2322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " title (InputLayer)             [(None, 10000)]      0           []                               \n",
      "                                                                                                  \n",
      " text_body (InputLayer)         [(None, 10000)]      0           []                               \n",
      "                                                                                                  \n",
      " tags (InputLayer)              [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 20100)        0           ['title[0][0]',                  \n",
      "                                                                  'text_body[0][0]',              \n",
      "                                                                  'tags[0][0]']                   \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 64)           1286464     ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " priority (Dense)               (None, 1)            65          ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " department (Dense)             (None, 4)            260         ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,286,789\n",
      "Trainable params: 1,286,789\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfb01ee",
   "metadata": {},
   "source": [
    "Training a model by providing lists of input and target arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8a7ed67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 2s 19ms/step - loss: 41.3857 - priority_loss: 0.3416 - department_loss: 41.0442 - priority_mean_absolute_error: 0.5086 - department_accuracy: 0.2109\n",
      "40/40 [==============================] - 1s 6ms/step - loss: 16.5486 - priority_loss: 0.3477 - department_loss: 16.2010 - priority_mean_absolute_error: 0.5142 - department_accuracy: 0.0789\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_samples = 1280\n",
    "\n",
    "title_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))\n",
    "text_body_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))\n",
    "tags_data = np.random.randint(0, 2, size=(num_samples, num_tags))\n",
    "\n",
    "priority_data = np.random.random(size=(num_samples, 1))\n",
    "department_data = np.random.randint(0, 2, size=(num_samples, num_departments))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss=['mean_squared_error', 'categorical_crossentropy'],\n",
    "              metrics=[['mean_absolute_error'],['accuracy']])\n",
    "model.fit([title_data, text_body_data, tags_data],[priority_data,department_data], epochs=1)\n",
    "model.evaluate([title_data, text_body_data, tags_data],\n",
    "               [priority_data, department_data])\n",
    "priority_preds, department_preds = model.predict([title_data, text_body_data, tags_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b52aba",
   "metadata": {},
   "source": [
    "Training a model by providing dicts of input & target arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb03a077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 1s 18ms/step - loss: 55.2510 - priority_loss: 0.3477 - department_loss: 54.9033 - priority_mean_absolute_error: 0.5142 - department_accuracy: 0.2508\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 52.2364 - priority_loss: 0.3477 - department_loss: 51.8887 - priority_mean_absolute_error: 0.5142 - department_accuracy: 0.5680\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss={\"priority\": \"mean_squared_error\", \"department\": \"categorical_crossentropy\"},\n",
    "              metrics={\"priority\": [\"mean_absolute_error\"], \"department\": [\"accuracy\"]})\n",
    "model.fit({\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data},\n",
    "          {\"priority\": priority_data, \"department\": department_data},\n",
    "          epochs=1)\n",
    "model.evaluate({\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data},\n",
    "               {\"priority\": priority_data, \"department\": department_data})\n",
    "priority_preds, department_preds = model.predict(\n",
    "    {\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4aeffe7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0eb7ecef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x7fd5e91e60d0>,\n",
       " <keras.engine.input_layer.InputLayer at 0x7fd5e91e6130>,\n",
       " <keras.engine.input_layer.InputLayer at 0x7fd5e91e6100>,\n",
       " <keras.layers.merge.Concatenate at 0x7fd5e91e6610>,\n",
       " <keras.layers.core.dense.Dense at 0x7fd5e91e6940>,\n",
       " <keras.layers.core.dense.Dense at 0x7fd5e91e6cd0>,\n",
       " <keras.layers.core.dense.Dense at 0x7fd5e91e3490>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23463034",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[3].input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea4b455",
   "metadata": {},
   "source": [
    "Creating new model by reusing intermediate layer outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8409243c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = model.layers[4].output\n",
    "difficulty = layers.Dense(3, activation='softmax', name='difficulty')(features)\n",
    "\n",
    "new_model = keras.Model(inputs = [title, text_body, tags],\n",
    "                       outputs = [priority, department, difficulty])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b73759",
   "metadata": {},
   "source": [
    "Subclassed Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2beea75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomerTicketModel(keras.Model):\n",
    "\n",
    "    def __init__(self, num_departments):\n",
    "        super().__init__()\n",
    "        self.concat_layer = layers.Concatenate()\n",
    "        self.mixing_layer = layers.Dense(64, activation='relu')\n",
    "        self.priority_scorer = layers.Dense(1, activation='sigmoid')\n",
    "        self.department_classifier = layers.Dense(\n",
    "            num_departments, activation='softmax')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        title = inputs['title']\n",
    "        text_body = inputs['text_body']\n",
    "        tags = inputs['tags']\n",
    "\n",
    "        features = self.concat_layer([title, text_body, tags])\n",
    "        features = self.mixing_layer(features)\n",
    "        priority = self.priority_scorer(features)\n",
    "        department = self.department_classifier(features)\n",
    "        return priority, department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28caba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomerTicketModel(num_departments=4)\n",
    "\n",
    "priority, department = model(\n",
    "    {'title': title_data, 'text_body': text_body_data, 'tags': tags_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cdc917f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 1s 18ms/step - loss: 17.8245 - output_1_loss: 0.3442 - output_2_loss: 17.4803 - output_1_mean_absolute_error: 0.5108 - output_2_accuracy: 0.2680\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 14.2736 - output_1_loss: 0.3477 - output_2_loss: 13.9259 - output_1_mean_absolute_error: 0.5142 - output_2_accuracy: 0.5672\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss=['mean_squared_error', 'categorical_crossentropy'],\n",
    "              metrics=[['mean_absolute_error'], ['accuracy']])\n",
    "model.fit({'title': title_data,\n",
    "           'text_body': text_body_data,\n",
    "           'tags': tags_data},\n",
    "          [priority_data, department_data],\n",
    "          epochs=1)\n",
    "model.evaluate({'title': title_data,\n",
    "                'text_body': text_body_data,\n",
    "                'tags': tags_data},\n",
    "               [priority_data, department_data])\n",
    "priority_preds, department_preds = model.predict({'title': title_data,\n",
    "                                                  'text_body': text_body_data,\n",
    "                                                  'tags': tags_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b94f5b",
   "metadata": {},
   "source": [
    "### Mixing and matching different components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914edeb4",
   "metadata": {},
   "source": [
    "Functional model including subclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "743d313c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(keras.Model):\n",
    "\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        if num_classes == 2:\n",
    "            num_units = 1\n",
    "            activation = 'sigmoid'\n",
    "        else:\n",
    "            num_units = num_classes\n",
    "            activation = 'softmax'\n",
    "        self.dense = layers.Dense(num_units, activation=activation)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.dense(inputs)\n",
    "\n",
    "inputs = keras.Input(shape=(3,))\n",
    "features = layers.Dense(64, activation='relu')(inputs)\n",
    "outputs = Classifier(num_classes=10)(features)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa90fb53",
   "metadata": {},
   "source": [
    "Subclassed model including Functional model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8870fbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(64,))\n",
    "outputs = layers.Dense(1, activation='sigmoid')(inputs)\n",
    "binary_classifier = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "class MyModel(keras.Model):\n",
    "\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.dense = layers.Dense(64, activation='relu')\n",
    "        self.classifier = binary_classifier\n",
    "\n",
    "    def call(self, inputs):\n",
    "        features = self.dense(inputs)\n",
    "        return self.classifier(features)\n",
    "\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c866eb",
   "metadata": {},
   "source": [
    "### Using built-in training and evaluation loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5e5fac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.2967 - accuracy: 0.9117 - val_loss: 0.1441 - val_accuracy: 0.9575\n",
      "Epoch 2/3\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.1645 - accuracy: 0.9540 - val_loss: 0.1219 - val_accuracy: 0.9679\n",
      "Epoch 3/3\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.1378 - accuracy: 0.9632 - val_loss: 0.1126 - val_accuracy: 0.9725\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1028 - accuracy: 0.9735\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "def get_mnist_model():\n",
    "    inputs = keras.Input(shape=(28*28,))\n",
    "    features = layers.Dense(512, activation='relu')(inputs)\n",
    "    features = layers.Dropout(0.5)(features)\n",
    "    outputs = layers.Dense(10, activation='softmax')(features)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "(images, labels), (test_images, test_labels) = mnist.load_data()\n",
    "images = images.reshape((60000, 28 * 28)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28)).astype('float32') / 255\n",
    "train_images, val_images = images[10000:], images[:10000]\n",
    "train_labels, val_labels = labels[10000:], labels[:10000]\n",
    "\n",
    "model = get_mnist_model()\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels,\n",
    "          epochs=3,\n",
    "          validation_data=(val_images, val_labels))\n",
    "test_metrics = model.evaluate(test_images, test_labels)\n",
    "predictions = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70620ad",
   "metadata": {},
   "source": [
    "Writing your own metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "972f4b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class RootMeanSquaredError(keras.metrics.Metric):\n",
    "\n",
    "    def __init__(self, name='rmse', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.mse_sum = self.add_weight(name='mse_sum', initializer='zeros')\n",
    "        self.total_samples = self.add_weight(\n",
    "            name='total_samples', initializer='zeros', dtype='int32')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.one_hot(y_true, depth=tf.shape(y_pred)[1])\n",
    "        mse = tf.reduce_sum(tf.square(y_true - y_pred))\n",
    "        self.mse_sum.assign_add(mse)\n",
    "        num_samples = tf.shape(y_pred)[0]\n",
    "        self.total_samples.assign_add(num_samples)\n",
    "\n",
    "    def result(self):\n",
    "        return tf.sqrt(self.mse_sum / tf.cast(self.total_samples, tf.float32))\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.mse_sum.assign(0.)\n",
    "        self.total_samples.assign(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "690ca44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.2919 - accuracy: 0.9145 - rmse: 7.1840 - val_loss: 0.1599 - val_accuracy: 0.9538 - val_rmse: 7.3601\n",
      "Epoch 2/3\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.1622 - accuracy: 0.9535 - rmse: 7.3569 - val_loss: 0.1299 - val_accuracy: 0.9633 - val_rmse: 7.4031\n",
      "Epoch 3/3\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.1370 - accuracy: 0.9634 - rmse: 7.3883 - val_loss: 0.1100 - val_accuracy: 0.9719 - val_rmse: 7.4189\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1016 - accuracy: 0.9738 - rmse: 7.4315\n"
     ]
    }
   ],
   "source": [
    "model = get_mnist_model()\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy', RootMeanSquaredError()])\n",
    "model.fit(train_images,train_labels,epochs=3, validation_data=(val_images,val_labels))\n",
    "test_metrics = model.evaluate(test_images,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976208f7",
   "metadata": {},
   "source": [
    "Using Callback argument in the fit() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30cb0906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 0.2984 - accuracy: 0.9111 - val_loss: 0.1478 - val_accuracy: 0.9577\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.1646 - accuracy: 0.9536 - val_loss: 0.1170 - val_accuracy: 0.9681\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 12s 8ms/step - loss: 0.1393 - accuracy: 0.9624 - val_loss: 0.1129 - val_accuracy: 0.9718\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.1245 - accuracy: 0.9672 - val_loss: 0.1158 - val_accuracy: 0.9727\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 11s 7ms/step - loss: 0.1187 - accuracy: 0.9699 - val_loss: 0.1092 - val_accuracy: 0.9737\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.1134 - accuracy: 0.9736 - val_loss: 0.1086 - val_accuracy: 0.9767\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 0.1049 - accuracy: 0.9755 - val_loss: 0.1057 - val_accuracy: 0.9786\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.1024 - accuracy: 0.9758 - val_loss: 0.1194 - val_accuracy: 0.9760\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0937 - accuracy: 0.9776 - val_loss: 0.1141 - val_accuracy: 0.9787\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0929 - accuracy: 0.9794 - val_loss: 0.1167 - val_accuracy: 0.9778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd5d8e009d0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=2,\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "    filepath='checkpoint_path.keras',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    )\n",
    "]\n",
    "model = get_mnist_model()\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "model.fit(train_images,train_labels,\n",
    "         epochs=10, callbacks=callbacks_list,\n",
    "         validation_data=(val_images,val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3452755e",
   "metadata": {},
   "source": [
    "Writing your own callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35c94d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs):\n",
    "        self.per_batch_losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.per_batch_losses.append(logs.get(\"loss\"))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        plt.clf()\n",
    "        plt.plot(range(len(self.per_batch_losses)), self.per_batch_losses,\n",
    "                 label=\"Training loss for each batch\")\n",
    "        plt.xlabel(f\"Batch (epoch {epoch})\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"plot_at_epoch_{epoch}\")\n",
    "        self.per_batch_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f6257df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 12s 7ms/step - loss: 0.2931 - accuracy: 0.9131 - val_loss: 0.1573 - val_accuracy: 0.9575\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 10s 6ms/step - loss: 0.1685 - accuracy: 0.9533 - val_loss: 0.1295 - val_accuracy: 0.9651\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.1389 - accuracy: 0.9627 - val_loss: 0.1179 - val_accuracy: 0.9708\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.1296 - accuracy: 0.9669 - val_loss: 0.1118 - val_accuracy: 0.9738\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.1141 - accuracy: 0.9710 - val_loss: 0.1126 - val_accuracy: 0.9741\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.1107 - accuracy: 0.9730 - val_loss: 0.1102 - val_accuracy: 0.9768\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.1069 - accuracy: 0.9752 - val_loss: 0.1161 - val_accuracy: 0.9759\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.1042 - accuracy: 0.9756 - val_loss: 0.1103 - val_accuracy: 0.9775\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0973 - accuracy: 0.9779 - val_loss: 0.1202 - val_accuracy: 0.9780\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 0.0932 - accuracy: 0.9783 - val_loss: 0.1167 - val_accuracy: 0.9796\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd5b03ec520>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = get_mnist_model()\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(train_images, train_labels,\n",
    "          epochs=10,\n",
    "          callbacks=[LossHistory()],\n",
    "          validation_data=(val_images, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecde2d19",
   "metadata": {},
   "source": [
    "Monitoring and visualization with Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3de76eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-17 19:42:11.442720: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at summary_kernels.cc:65 : PERMISSION_DENIED: /full_path_to_your_log_dir; Read-only file system\n"
     ]
    },
    {
     "ename": "PermissionDeniedError",
     "evalue": "/full_path_to_your_log_dir; Read-only file system [Op:CreateSummaryFileWriter]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionDeniedError\u001b[0m                     Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrmsprop\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m               loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      6\u001b[0m tensorboard \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mTensorBoard(\n\u001b[1;32m      7\u001b[0m     log_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/full_path_to_your_log_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtensorboard\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:7107\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7106\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7107\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mPermissionDeniedError\u001b[0m: /full_path_to_your_log_dir; Read-only file system [Op:CreateSummaryFileWriter]"
     ]
    }
   ],
   "source": [
    "model = get_mnist_model()\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(\n",
    "    log_dir=\"/full_path_to_your_log_dir\",\n",
    ")\n",
    "model.fit(train_images, train_labels,\n",
    "          epochs=10,\n",
    "          validation_data=(val_images, val_labels),\n",
    "          callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5487f8b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-12deb6018d7103b2\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-12deb6018d7103b2\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /full_path_to_your_log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab454bf4",
   "metadata": {},
   "source": [
    "### Writing your own training and evaluation loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06bdc0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: 1.00\n"
     ]
    }
   ],
   "source": [
    "metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "targets = [0, 1, 2]\n",
    "predictions = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
    "metric.update_state(targets, predictions)\n",
    "current_result = metric.result()\n",
    "print(f'result: {current_result:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f55a519d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of values: 2.00\n"
     ]
    }
   ],
   "source": [
    "values = [0, 1, 2, 3, 4]\n",
    "mean_tracker = keras.metrics.Mean()\n",
    "for value in values:\n",
    "    mean_tracker.update_state(value)\n",
    "print(f'Mean of values: {mean_tracker.result():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e8e923",
   "metadata": {},
   "source": [
    "Writing step-by-step training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9921c24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_mnist_model()\n",
    "\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = keras.optimizers.RMSprop()\n",
    "metrics = [keras.metrics.SparseCategoricalAccuracy()]\n",
    "loss_tracking_metric = keras.metrics.Mean()\n",
    "\n",
    "def train_step(inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        loss = loss_fn(targets, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "\n",
    "    logs = {}\n",
    "    for metric in metrics:\n",
    "        metric.update_state(targets, predictions)\n",
    "        logs[metric.name] = metric.result()\n",
    "\n",
    "    loss_tracking_metric.update_state(loss)\n",
    "    logs['loss'] = loss_tracking_metric.result()\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "167c8b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_metrics():\n",
    "    for metric in metrics:\n",
    "        metric.reset_state()\n",
    "    loss_tracking_metric.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "050a2286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at the end of epoch 0\n",
      "...sparse_categorical_accuracy: 0.9138\n",
      "...loss: 0.2891\n",
      "Results at the end of epoch 1\n",
      "...sparse_categorical_accuracy: 0.9546\n",
      "...loss: 0.1665\n",
      "Results at the end of epoch 2\n",
      "...sparse_categorical_accuracy: 0.9625\n",
      "...loss: 0.1409\n"
     ]
    }
   ],
   "source": [
    "training_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "training_dataset = training_dataset.batch(32)\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    reset_metrics()\n",
    "    for inputs_batch, targets_batch in training_dataset:\n",
    "        logs = train_step(inputs_batch, targets_batch)\n",
    "    print(f'Results at the end of epoch {epoch}')\n",
    "    for key, value in logs.items():\n",
    "        print(f'...{key}: {value:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "019b5bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results:\n",
      "...val_sparse_categorical_accuracy: 0.9669\n",
      "...val_loss: 0.1325\n",
      "CPU times: user 2.13 s, sys: 77.8 ms, total: 2.21 s\n",
      "Wall time: 1.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def test_step(inputs, targets):\n",
    "    predictions = model(inputs, training=False)\n",
    "    loss = loss_fn(targets, predictions)\n",
    "\n",
    "    logs = {}\n",
    "    for metric in metrics:\n",
    "        metric.update_state(targets, predictions)\n",
    "        logs['val_' + metric.name] = metric.result()\n",
    "\n",
    "    loss_tracking_metric.update_state(loss)\n",
    "    logs['val_loss'] = loss_tracking_metric.result()\n",
    "    return logs\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
    "val_dataset = val_dataset.batch(32)\n",
    "reset_metrics()\n",
    "for inputs_batch, targets_batch in val_dataset:\n",
    "    logs = test_step(inputs_batch, targets_batch)\n",
    "print('Evaluation results:')\n",
    "for key, value in logs.items():\n",
    "    print(f'...{key}: {value:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a589a7",
   "metadata": {},
   "source": [
    "Make it fast with tf.function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5a43e4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function test_step at 0x7fd5b43ad160> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function test_step at 0x7fd5b43ad160>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function test_step at 0x7fd5b43ad160> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function test_step at 0x7fd5b43ad160>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Evaluation results:\n",
      "...val_sparse_categorical_accuracy: 0.9669\n",
      "...val_loss: 0.1325\n",
      "CPU times: user 1.01 s, sys: 95.4 ms, total: 1.11 s\n",
      "Wall time: 745 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "@tf.function\n",
    "def test_step(inputs, targets):\n",
    "    predictions = model(inputs, training=False)\n",
    "    loss = loss_fn(targets, predictions)\n",
    "\n",
    "    logs = {}\n",
    "    for metric in metrics:\n",
    "        metric.update_state(targets, predictions)\n",
    "        logs['val_' + metric.name] = metric.result()\n",
    "\n",
    "    loss_tracking_metric.update_state(loss)\n",
    "    logs['val_loss'] = loss_tracking_metric.result()\n",
    "    return logs\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
    "val_dataset = val_dataset.batch(32)\n",
    "reset_metrics()\n",
    "for inputs_batch, targets_batch in val_dataset:\n",
    "    logs = test_step(inputs_batch, targets_batch)\n",
    "print('Evaluation results:')\n",
    "for key, value in logs.items():\n",
    "    print(f'...{key}: {value:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812ccb38",
   "metadata": {},
   "source": [
    "Leveraging fit() with custom training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec2cef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
